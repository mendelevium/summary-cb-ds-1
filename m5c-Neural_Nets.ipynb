{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of models\n",
    "\n",
    "A **Multi-Layer Perceptron** (MLP) is simply a neural network feeding inputs into outputs through only \"dense\" layers (fully connected layers).\n",
    "\n",
    "A **Feed Forward Network** (FFNN) is a generalization of the MLP that uses some other types of layers (dropout, batchnorm, etc) to enhance performance.\n",
    "\n",
    "An **AutoEncoder** maps $X$ back to itself with a smaller layer in the middle (which compresses the information). You can think of it as an *embedding* (eg. dimensionality reduction) technique.\n",
    "\n",
    "For now we'll only focus on pure **feedforward** networks, we'll look at **sequence learning** (Recurrent Nets and Transformer Architectures) later on in the course.\n",
    "\n",
    "This means we'll focus oon green and purple nodes in this chart:\n",
    "\n",
    "![](docs/architecture_types.png)\n",
    "\n",
    "## Popular Layer types\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Is similar in idea to bagging that we saw for random forests.\n",
    "\n",
    "The `Dropout` layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by `1/(1 - rate)` such that the sum over all inputs is unchanged.\n",
    "\n",
    "### BatchNorm\n",
    "\n",
    "Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n",
    "\n",
    "This normalizes input data per-batch.\n",
    "\n",
    "Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, reducing the need for Dropout.\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n",
    "\n",
    "This layer can only be used as the first layer in a model. It's used to automatically turn categorical data into dense vectors bypassing the need for one-hot encoding.\n",
    "\n",
    "We won't need this one for image data.\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "Is the SGD parameter. \n",
    "\n",
    "If the Batch size is the size of your dataset, then you're doing classical gradient descent rather than SGD. Often smaller batches can be *better* because they're noisier and hence escape local minima more easily."
   ]
  },
  {
   "source": [
    "# Feed Forward Neural Networks (FFNN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (None, 784)\n",
    "\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# convert class vectors to one-hot class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               78500     \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 100)               400       \n_________________________________________________________________\nactivation (Activation)      (None, 100)               0         \n_________________________________________________________________\ndropout (Dropout)            (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               10100     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 100)               0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 100)               10100     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 100)               400       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 100)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                1010      \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 10)                40        \n_________________________________________________________________\nactivation_3 (Activation)    (None, 10)                0         \n=================================================================\nTotal params: 100,950\nTrainable params: 100,330\nNon-trainable params: 620\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Activation\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Note the output of this layer is very large: 100 dimensions\n",
    "model.add(Dense(100, input_dim=784))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Hidden layer    \n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Hidden layer\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Note the output shape is the number of classes\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "330/330 [==============================] - 8s 16ms/step - loss: 1.7305 - accuracy: 0.4425 - val_loss: 0.5314 - val_accuracy: 0.9107\n",
      "Epoch 2/10\n",
      "330/330 [==============================] - 3s 9ms/step - loss: 0.8118 - accuracy: 0.8097 - val_loss: 0.3326 - val_accuracy: 0.9303\n",
      "Epoch 3/10\n",
      "330/330 [==============================] - 3s 9ms/step - loss: 0.6103 - accuracy: 0.8541 - val_loss: 0.2520 - val_accuracy: 0.9412\n",
      "Epoch 4/10\n",
      "330/330 [==============================] - 3s 9ms/step - loss: 0.4960 - accuracy: 0.8780 - val_loss: 0.2094 - val_accuracy: 0.9478\n",
      "Epoch 5/10\n",
      "330/330 [==============================] - 4s 11ms/step - loss: 0.4368 - accuracy: 0.8929 - val_loss: 0.1799 - val_accuracy: 0.9522\n",
      "Epoch 6/10\n",
      "330/330 [==============================] - 4s 13ms/step - loss: 0.3900 - accuracy: 0.9023 - val_loss: 0.1565 - val_accuracy: 0.9582\n",
      "Epoch 7/10\n",
      "330/330 [==============================] - 4s 12ms/step - loss: 0.3558 - accuracy: 0.9108 - val_loss: 0.1458 - val_accuracy: 0.9608\n",
      "Epoch 8/10\n",
      "330/330 [==============================] - 4s 11ms/step - loss: 0.3265 - accuracy: 0.9175 - val_loss: 0.1405 - val_accuracy: 0.9635\n",
      "Epoch 9/10\n",
      "330/330 [==============================] - 4s 12ms/step - loss: 0.3087 - accuracy: 0.9218 - val_loss: 0.1336 - val_accuracy: 0.9638\n",
      "Epoch 10/10\n",
      "330/330 [==============================] - 4s 13ms/step - loss: 0.2870 - accuracy: 0.9248 - val_loss: 0.1235 - val_accuracy: 0.9661\n",
      "Loss: 13.92\n",
      "Accuracy: 96.03\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, \n",
    "                    epochs=epochs, validation_split=0.1)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Loss: {(scores[0]*100):.2f}')\n",
    "print(f'Accuracy: {(scores[1]*100):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "source": [
    "# Convolutional Neural Networks (CNN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 73s 168ms/step - loss: 0.7662 - accuracy: 0.7612 - val_loss: 0.0799 - val_accuracy: 0.9790\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 53s 126ms/step - loss: 0.1227 - accuracy: 0.9614 - val_loss: 0.0548 - val_accuracy: 0.9847\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 62s 148ms/step - loss: 0.0891 - accuracy: 0.9730 - val_loss: 0.0452 - val_accuracy: 0.9878\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 71s 168ms/step - loss: 0.0707 - accuracy: 0.9789 - val_loss: 0.0389 - val_accuracy: 0.9895\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 57s 135ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.0391 - val_accuracy: 0.9898\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 64s 152ms/step - loss: 0.0543 - accuracy: 0.9838 - val_loss: 0.0356 - val_accuracy: 0.9917\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 68s 160ms/step - loss: 0.0541 - accuracy: 0.9831 - val_loss: 0.0341 - val_accuracy: 0.9900\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 50s 119ms/step - loss: 0.0441 - accuracy: 0.9864 - val_loss: 0.0336 - val_accuracy: 0.9910\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 64s 152ms/step - loss: 0.0438 - accuracy: 0.9858 - val_loss: 0.0301 - val_accuracy: 0.9920\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 70s 165ms/step - loss: 0.0402 - accuracy: 0.9875 - val_loss: 0.0296 - val_accuracy: 0.9922\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 53s 125ms/step - loss: 0.0358 - accuracy: 0.9885 - val_loss: 0.0299 - val_accuracy: 0.9923\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 65s 154ms/step - loss: 0.0353 - accuracy: 0.9883 - val_loss: 0.0305 - val_accuracy: 0.9925\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 58s 138ms/step - loss: 0.0348 - accuracy: 0.9888 - val_loss: 0.0293 - val_accuracy: 0.9920\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 50s 118ms/step - loss: 0.0349 - accuracy: 0.9877 - val_loss: 0.0289 - val_accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 56s 132ms/step - loss: 0.0318 - accuracy: 0.9893 - val_loss: 0.0333 - val_accuracy: 0.9920\n",
      "Loss: 2.56\n",
      "Accuracy: 99.14\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, \n",
    "                    epochs=epochs, validation_split=0.15)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Loss: {(scores[0]*100):.2f}')\n",
    "print(f'Accuracy: {(scores[1]*100):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "source": [
    "# Recurrent Neural Networks (RNN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/davidzhao365/review-sentiment-analysis-with-rnn-and-gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                 Review  Score  Score_10\n",
       "0     The Haunting is a film that boasts a really cr...      1         7\n",
       "1     This mindless movie is a piece of crap and bor...      0         1\n",
       "2     George Brent is a reporter sent to interview a...      0         4\n",
       "3     After sitting through this pile of dung, my hu...      0         1\n",
       "4     This movie is the next segment in the pokemon ...      1        10\n",
       "...                                                 ...    ...       ...\n",
       "6245  There's perhaps a special reason why The Fox a...      1         8\n",
       "6246  And I'm serious! Truly one of the most fantast...      1        10\n",
       "6247  H.G. Wells in 1936 was past his prime and the ...      1         9\n",
       "6248  Yet another venture into the realm of the teen...      0         1\n",
       "6249  National Lampoon's Class Reunion is a classic ...      1         9\n",
       "\n",
       "[6250 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Review</th>\n      <th>Score</th>\n      <th>Score_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Haunting is a film that boasts a really cr...</td>\n      <td>1</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This mindless movie is a piece of crap and bor...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>George Brent is a reporter sent to interview a...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>After sitting through this pile of dung, my hu...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>This movie is the next segment in the pokemon ...</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6245</th>\n      <td>There's perhaps a special reason why The Fox a...</td>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>6246</th>\n      <td>And I'm serious! Truly one of the most fantast...</td>\n      <td>1</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6247</th>\n      <td>H.G. Wells in 1936 was past his prime and the ...</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>6248</th>\n      <td>Yet another venture into the realm of the teen...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6249</th>\n      <td>National Lampoon's Class Reunion is a classic ...</td>\n      <td>1</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>6250 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "df = pd.read_csv('data/imdb_train.csv')\n",
    "df = df.sample(frac=.25).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\\\s\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z' ]\", \"\", text)\n",
    "    #text = text.split(' ')\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    preprocessor=clean_text,\n",
    "    ngram_range=(1, 2),\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import to_categorical\n",
    "vocab = 3000\n",
    "max_len = 500\n",
    "\n",
    "#X = tfidf.fit_transform(df[\"Review\"]).toarray()\n",
    "df['Review'] = df['Review'].apply(lambda x: clean_text(x))\n",
    "tokenizer = Tokenizer(num_words=vocab)\n",
    "tokenizer.fit_on_texts(df['Review'])\n",
    "X = tokenizer.texts_to_sequences(df['Review'])\n",
    "\n",
    "#y = to_categorical(df[\"Score\"])\n",
    "y = np.array(df[\"Score\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_4 (Embedding)      (None, 500, 100)          300000    \n_________________________________________________________________\ngru_5 (GRU)                  (None, 100)               60600     \n_________________________________________________________________\ndense_12 (Dense)             (None, 10)                1010      \n_________________________________________________________________\ndense_13 (Dense)             (None, 1)                 11        \n=================================================================\nTotal params: 361,621\nTrainable params: 361,621\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "# GRU Model\n",
    "num_classes = 1\n",
    "\n",
    "model = Sequential()\n",
    "# Input layer\n",
    "model.add(Embedding(vocab, 100, input_length=max_len))\n",
    "model.add(GRU(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "67/67 [==============================] - 237s 4s/step - loss: 0.2474 - accuracy: 0.8984 - val_loss: 0.4605 - val_accuracy: 0.8000\n",
      "Epoch 2/5\n",
      "67/67 [==============================] - 232s 3s/step - loss: 0.2185 - accuracy: 0.9153 - val_loss: 0.5373 - val_accuracy: 0.7987\n",
      "Epoch 3/5\n",
      "67/67 [==============================] - 216s 3s/step - loss: 0.1427 - accuracy: 0.9435 - val_loss: 0.5434 - val_accuracy: 0.8213\n",
      "Epoch 4/5\n",
      "67/67 [==============================] - 218s 3s/step - loss: 0.1025 - accuracy: 0.9624 - val_loss: 0.6113 - val_accuracy: 0.8147\n",
      "Epoch 5/5\n",
      "67/67 [==============================] - 227s 3s/step - loss: 0.0657 - accuracy: 0.9786 - val_loss: 0.7374 - val_accuracy: 0.8293\n",
      "Loss: 69.71\n",
      "Accuracy: 81.28\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Fit and evaluate\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, \n",
    "                    epochs=epochs, validation_split=0.15, verbose=1)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Loss: {(scores[0]*100):.2f}')\n",
    "yprint(f'Accuracy: {(scores[1]*100):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot();"
   ]
  },
  {
   "source": [
    "# Generative Adversarial Networks (RNN)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.datadriveninvestor.com/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A Gentle Introduction to PyTorch 1.2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python38364bitbasee0d3a3f774f446a2aff50e11b591e430",
   "display_name": "Python 3.8.3 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}