{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3-final"},"colab":{"name":"Math Functions.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"source":["# Natural Language Processing\n","\n","### Install Spacy\n","https://spacy.io/api/doc/\n","\n","```\n","conda install -c conda-forge spacy\n","python -m spacy download en_core_web_sm\n","python -m spacy download en_core_web_md\n","```\n","Optionnaly install french language core (fr_core_news_sm)\n","\n","### Install NLTK\n","https://www.nltk.org/\n","\n","```\n","conda install -c anaconda nltk\n","```"],"cell_type":"markdown","metadata":{"id":"5Yiopyo2Ihhk","colab_type":"text"}},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"]},"metadata":{},"execution_count":3}],"source":["# Pipelines are a series of operations to tag, parse and describe the data\n","nlp.pipe_names"]},{"source":["## Tokenisation\n","Split up all the component parts (words & punctuation) into \"tokens\". "],"cell_type":"markdown","metadata":{}},{"source":["### Token attributes\n","|Tag|Description|`doc2[0].tag`|\n","|:------|:------|:------|\n","|`.text`|The original word text<!-- .element: style=\"text-align:left;\" -->|`Tesla`|\n","|`.lemma_`|The base form of the word|`tesla`|\n","|`.pos_`|The simple part-of-speech tag|`PROPN`/`proper noun`|\n","|`.tag_`|The detailed part-of-speech tag|`NNP`/`noun, proper singular`|\n","|`.shape_`|The word shape â€“ capitalization, punctuation, digits|`Xxxxx`|\n","|`.is_alpha`|Is the token an alpha character?|`True`|\n","|`.is_stop`|Is the token part of a stop list, i.e. the most common words of the language?|`False`|"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla    \t PROPN \t nsubj\nis       \t AUX \t aux\nlooking  \t VERB \t ROOT\nat       \t ADP \t prep\nbuying   \t VERB \t pcomp\nU.S.     \t PROPN \t compound\nstartup  \t NOUN \t dobj\nfor      \t ADP \t prep\n$        \t SYM \t quantmod\n6        \t NUM \t compound\nmillion  \t NUM \t pobj\n"]}],"source":["# Create a Doc object\n","doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million') # u stand for unicode\n","\n","# Print each token separately\n","for token in doc:\n","    # print text, part of speach, dependancy\n","    #print(token.text, token.pos_, token.dep_)\n","    print(f'{token.text:{8}}', '\\t', token.pos_, '\\t', token.dep_) "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'proper noun'"]},"metadata":{},"execution_count":6}],"source":["# Get the details for abreviations\n","spacy.explain('PROPN')"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["This is the first sentence.\nThis is another sentence.\nThis is the last sentence.\n"]}],"source":["# Sentence segmentation\n","doc2 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n","for sent in doc11.sents:\n","    print(sent)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | \n----\nApple - ORG - Companies, agencies, institutions, etc.\nHong Kong - GPE - Countries, cities, states\n$6 million - MONEY - Monetary values, including unit\n"]}],"source":["# Name Entities\n","doc3 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n","\n","for token in doc3:\n","    print(token.text, end=' | ')\n","\n","print('\\n----')\n","\n","# Get entities\n","for ent in doc3.ents:\n","    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    the last quarter\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n</mark>\n \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Apple\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n sold \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    nearly 20 thousand\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    iPods\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n for a profit of \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    $6 million\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n</mark>\n.</div></span>"},"metadata":{}}],"source":["# Name Entities visualization\n","from spacy import displacy\n","doc5 = nlp(u'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.')\n","displacy.render(doc5, style='ent', jupyter=True)"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Over the last quarter \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Apple\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n sold nearly 20 thousand \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    iPods\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n for a profit of $6 million.</div></span>"},"metadata":{}}],"source":["options = { 'ents': ['ORG'] }\n","displacy.render(doc5, style='ent', jupyter=True, options=options)"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Autonomous cars\ninsurance liability\nmanufacturers\n"]}],"source":["# Noun chunks\n","doc4 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n","\n","for chunk in doc4.noun_chunks:\n","    print(chunk.text)"]},{"source":["## Stemming\n","Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n--Porter--\nrun --> run\nrunner --> runner\nrunning --> run\nran --> ran\nruns --> run\neasily --> easili\nfairly --> fairli\n\n--Snowball--\nrun --> run\nrunner --> runner\nrunning --> run\nran --> ran\nruns --> run\neasily --> easili\nfairly --> fair\n"]}],"source":["# Import the toolkit and the full Porter Stemmer library\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.stem.snowball import SnowballStemmer\n","\n","p_stemmer = PorterStemmer()\n","# The Snowball Stemmer requires that you pass a language parameter\n","s_stemmer = SnowballStemmer(language='english')\n","\n","words = ['run','runner','running','ran','runs','easily','fairly']\n","#words = ['generous','generation','generously','generate']\n","\n","print('\\n--Porter--')\n","for word in words:\n","    print(word+' --> '+p_stemmer.stem(word))\n","\n","print('\\n--Snowball--')\n","for word in words:\n","    print(word+' --> '+s_stemmer.stem(word))"]},{"source":["## Lemmatization\n","In contrast to stemming, lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a *morphological analysis* to words. The lemma of 'was' is 'be' and the lemma of 'mice' is 'mouse'."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["I            PRON   4690420944186131903    I\nam           AUX    10382539506755952630   be\na            DET    11901859001352538922   a\nrunner       NOUN   12640964157389618806   runner\nrunning      VERB   12767647472892411841   run\nin           ADP    3002984154512732771    in\na            DET    11901859001352538922   a\nrace         NOUN   8048469955494714898    race\nbecause      SCONJ  16950148841647037698   because\nI            PRON   4690420944186131903    I\nlove         VERB   3702023516439754181    love\nto           PART   3791531372978436496    to\nrun          VERB   12767647472892411841   run\nsince        SCONJ  10066841407251338481   since\nI            PRON   4690420944186131903    I\nran          VERB   12767647472892411841   run\ntoday        NOUN   11042482332948150395   today\n"]}],"source":["doc5 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n","\n","def show_lemmas(text):\n","    for token in doc5:\n","        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')\n","\n","show_lemmas(doc5)"]},{"source":["## Stop Words\n","Words like \"a\" and \"the\" appear frequently and doesn't carry meaning."],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'anywhere', 'hers', 'whom', 'does', 'less', 'ourselves', 'nâ€˜t', 'their', 'after', 'that', 'â€™s', 'them', 'together', 'among', 'further', 'â€˜m', 'made', 'â€˜s', 'of', 'anyway', 'name', 'another', 'yet', 'must', 'very', \"n't\", 'him', 'two', 'toward', 'themselves', 'rather', \"'s\", 'until', 'â€™ve', 'you', 'call', 'into', 'while', 'yours', 'however', \"'ve\", 'top', 'whereby', 'everyone', 'sometimes', 'which', 'otherwise', 'â€˜ve', 'some', 'so', 'ca', 'her', 'unless', 'my', 'due', 'thereby', 'side', 'become', 'no', 'most', 'every', 'eight', 'everything', 'whatever', 'seemed', 'three', 'whereupon', 'â€™d', 'hence', 'can', 'he', 'still', 'something', 'throughout', 'if', 'now', \"'re\", 'amount', 'many', 'per', 'in', \"'ll\", 'namely', 'i', 'am', 'about', 'show', 'â€™m', 'whereafter', 'should', 'nor', 'never', 'those', 'she', 'except', 'whether', 'without', 'thence', 'yourself', 'front', 'alone', 'than', 'became', 'why', 'your', 'anyhow', 'under', 'beforehand', 'somehow', 'once', 'had', 'nine', 'other', 'amongst', 'therefore', 'from', 'during', 'via', 'before', 'nowhere', 'ever', 'hereupon', 'doing', 'full', 'â€™ll', 'meanwhile', 'first', 'make', 'against', 'afterwards', 'is', 'behind', 'on', 'off', 'would', 'various', 'down', 'thus', 'done', 'do', 'others', 'moreover', 'who', 'sometime', 'onto', 'really', 'well', 'same', 'just', 'mostly', 'between', 'hereafter', 'say', 'upon', 'too', 'our', 'anyone', 'the', 'last', 'sixty', 'within', 'again', 'be', 'take', 'everywhere', 'twelve', 'beside', 'here', 'cannot', 'only', 'get', 'whole', 'will', 'noone', 'eleven', 'both', 'formerly', 'nothing', 'although', 'across', 'though', 'through', 'any', 'somewhere', 'more', 'much', 'forty', 'with', 'myself', 'nâ€™t', 'below', 'did', 'have', 'bottom', 'former', 'seems', 'move', 'they', 'it', 'whereas', 'but', 'over', 'five', 'whoever', 'go', 'this', 'us', 'nevertheless', 'yourselves', 'a', 'anything', 'regarding', 'quite', 'serious', 'as', 'there', 'up', 'when', 'besides', 'thereafter', 'or', 'several', 'me', 'â€™re', 'not', 'may', 'â€˜d', \"'m\", 'whenever', 'since', 'latterly', 'â€˜ll', 'few', 'out', 'fifteen', 'â€˜re', 'his', 'also', 'hereby', 'whither', 'always', 'see', 'else', 'along', 'mine', 'therein', 'latter', 'how', 'herein', 'nobody', 'almost', 'been', 'least', 'towards', 'thru', 'four', 'whose', 'already', 'neither', 'were', 'keep', 'what', 'an', 'and', 'empty', 'fifty', 'enough', 'seeming', 'ours', 'its', 'becomes', 'herself', 'such', 'becoming', 'at', 'has', 'please', 'wherever', 'ten', 'itself', 'thereupon', 'then', 'even', 're', 'we', 'above', 'whence', 'someone', 'by', 'hundred', 'wherein', 'because', 'seem', 'for', 'around', 'elsewhere', 'twenty', 'all', 'these', 'might', 'give', 'own', 'next', 'using', 'one', 'either', 'put', 'often', 'being', 'third', 'where', 'himself', 'part', 'are', 'back', 'btw', 'indeed', 'used', 'perhaps', 'was', 'six', 'none', 'to', 'each', 'could', \"'d\"}\n"]}],"source":["print(nlp.Defaults.stop_words)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":52}],"source":["nlp.vocab['myself'].is_stop"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["# Add the word to the set of stop words. Use lowercase!\n","nlp.Defaults.stop_words.add('btw')\n","# Set the stop_word tag on the lexeme\n","nlp.vocab['btw'].is_stop = True\n","\n","# Remove the word from the set of stop words\n","nlp.Defaults.stop_words.remove('beyond')\n","# Remove the stop_word tag from the lexeme\n","nlp.vocab['beyond'].is_stop = False"]},{"source":["## Matching\n","spaCy offers a rule-matching tool called `Matcher` that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. https://spacy.io/usage/rule-based-matching"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["# Import the Matcher library\n","from spacy.matcher import Matcher\n","matcher = Matcher(nlp.vocab)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[(8656102463236116519, 1, 3), (8656102463236116519, 13, 16)]\n"]}],"source":["# Solar Power\n","pattern1 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n","# Solar-Power\n","pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}] # * = any number of times\n","\n","matcher.add('SolarPower', [pattern1, pattern2])\n","\n","doc6 = nlp(u'The Solar Power industry continues to grow as demand for solarpower increases. Solar--power cars are gaining popularity.')\n","\n","found_matches = matcher(doc6)\n","print(found_matches)"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["8656102463236116519 SolarPower 1 3 Solar Power\n8656102463236116519 SolarPower 10 11 solarpower\n8656102463236116519 SolarPower 13 16 Solar--power\n"]}],"source":["for match_id, start, end in found_matches:\n","    string_id = nlp.vocab.strings[match_id]  # get string representation\n","    span = doc[start:end]                    # get the matched span\n","    print(match_id, string_id, start, end, span.text)"]},{"source":["## Phrase Matching\n","is basicaly matching of a list"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["# Import the PhraseMatcher library\n","from spacy.matcher import PhraseMatcher\n","pmatcher = PhraseMatcher(nlp.vocab)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(3473369816841043438, 41, 45),\n"," (3473369816841043438, 49, 53),\n"," (3473369816841043438, 54, 56),\n"," (3473369816841043438, 61, 65),\n"," (3473369816841043438, 673, 677),\n"," (3473369816841043438, 2987, 2991)]"]},"metadata":{},"execution_count":87}],"source":["with open('data/reaganomics.txt') as f:\n","    doc7 = nlp(f.read())\n","\n","# First, create a list of match phrases:\n","phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n","\n","# Next, convert each phrase to a Doc object:\n","phrase_patterns = [nlp(text) for text in phrase_list]\n","\n","# Pass each Doc object into matcher (note the use of the asterisk!):\n","pmatcher.add('VoodooEconomics', None, *phrase_patterns)\n","\n","# Build a list of matches:\n","matches = pmatcher(doc7)\n","\n","# (match_id, start, end)\n","matches"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["3473369816841043438 VoodooEconomics 41 45 supply-side economics\n3473369816841043438 VoodooEconomics 49 53 trickle-down economics\n3473369816841043438 VoodooEconomics 54 56 voodoo economics\n3473369816841043438 VoodooEconomics 61 65 free-market economics\n3473369816841043438 VoodooEconomics 673 677 supply-side economics\n3473369816841043438 VoodooEconomics 2987 2991 trickle-down economics\n"]}],"source":["for match_id, start, end in matches:\n","    string_id = nlp.vocab.strings[match_id]  # get string representation\n","    span = doc7[start:end]                    # get the matched span\n","    print(match_id, string_id, start, end, span.text)"]},{"source":["## Counting POS Tags"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["84. ADJ  : 3\n85. ADP  : 1\n90. DET  : 2\n92. NOUN : 3\n94. PART : 1\n97. PUNCT: 1\n100. VERB : 1\n"]}],"source":["doc8 = nlp(u\"The quick brown fox jumped over the lazy dog's back.\")\n","\n","# Count the frequencies of different coarse-grained POS tags:\n","POS_counts = doc8.count_by(spacy.attrs.POS)\n","POS_counts\n","\n","for k,v in sorted(POS_counts.items()):\n","    print(f'{k}. {doc8.vocab[k].text:{5}}: {v}')"]},{"source":["## Name Entity Regognition"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Washington, DC - GPE - Countries, cities, states\nnext May - DATE - Absolute or relative dates or periods\nthe Washington Monument - ORG - Companies, agencies, institutions, etc.\n"]}],"source":["# Write a function to display basic entity info:\n","def show_ents(doc):\n","    if doc.ents:\n","        for ent in doc.ents:\n","            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n","    else:\n","        print('No named entities found.')\n","\n","doc9 = nlp(u'May I go to Washington, DC next May to see the Washington Monument?')\n","\n","show_ents(doc9)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["U.K. - GPE - Countries, cities, states\n$6 million - MONEY - Monetary values, including unit\nNone\n"]}],"source":["# Tesla not recognize as an entity\n","doc10 = nlp(u'Tesla to build a U.K. factory for $6 million')\n","print(show_ents(doc10))"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Tesla - ORG - Companies, agencies, institutions, etc.\nU.K. - GPE - Countries, cities, states\n$6 million - MONEY - Monetary values, including unit\nNone\n"]}],"source":["# Add entities\n","from spacy.tokens import Span\n","\n","# Get the hash value of the ORG entity label\n","ORG = doc10.vocab.strings[u'ORG']\n","\n","# Create a Span for the new entity\n","new_ent = Span(doc10, 0, 1, label=ORG)\n","\n","# Add the entity to the existing Doc object\n","doc10.ents = list(doc10.ents) + [new_ent]\n","\n","# Verify\n","print(show_ents(doc10))"]},{"source":["## Features extraction"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  label                                            message  length  punct\n","0   ham  Go until jurong point, crazy.. Available only ...     111      9\n","1   ham                      Ok lar... Joking wif u oni...      29      6\n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n","3   ham  U dun say so early hor... U c already then say...      49      6\n","4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>message</th>\n      <th>length</th>\n      <th>punct</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>111</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>29</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>155</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>49</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>61</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":140}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('data/smsspamcollection.tsv', sep='\\t')\n","X = df['message']\n","y = df['label']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# spam or ham dataset\n","df.head()"]},{"cell_type":"code","execution_count":138,"metadata":{},"outputs":[],"source":["# Create pipeline\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.svm import LinearSVC\n","\n","text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n","                     ('clf', LinearSVC()),\n","])\n","\n","# Feed the training data through the pipeline\n","text_clf.fit(X_train, y_train)  \n","\n","# Form a prediction set\n","predictions = text_clf.predict(X_test)"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["[[1586    7]\n [  12  234]]\n              precision    recall  f1-score   support\n\n         ham       0.99      1.00      0.99      1593\n        spam       0.97      0.95      0.96       246\n\n    accuracy                           0.99      1839\n   macro avg       0.98      0.97      0.98      1839\nweighted avg       0.99      0.99      0.99      1839\n\n0.989668297988037\n"]}],"source":["# Report the confusion matrix\n","from sklearn import metrics\n","\n","print(metrics.confusion_matrix(y_test,predictions))\n","# Print a classification report\n","print(metrics.classification_report(y_test,predictions))\n","# Print the overall accuracy\n","print(metrics.accuracy_score(y_test,predictions))"]},{"source":["## Word Vectors"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20000, 300)"]},"metadata":{},"execution_count":150}],"source":["nlp = spacy.load('en_core_web_md')\n","nlp.vocab.vectors.shape"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["lion lion 1.0\nlion cat 0.5265437\nlion pet 0.39923772\ncat lion 0.5265437\ncat cat 1.0\ncat pet 0.7505456\npet lion 0.39923772\npet cat 0.7505456\npet pet 1.0\n"]}],"source":["# Create a three-token Doc object:\n","tokens = nlp(u'lion cat pet')\n","\n","# Iterate through token combinations:\n","for token1 in tokens:\n","    for token2 in tokens:\n","        print(token1.text, token2.text, token1.similarity(token2))"]},{"cell_type":"code","execution_count":152,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["['king', 'woman', 'she', 'lion', 'who', 'when', 'dare', 'cat', 'was', 'not']\n"]}],"source":["from scipy import spatial\n","\n","cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n","\n","king = nlp.vocab['king'].vector\n","man = nlp.vocab['man'].vector\n","woman = nlp.vocab['woman'].vector\n","\n","# Now we find the closest vector in the vocabulary \n","# to the result of \"man\" - \"woman\" + \"queen\"\n","new_vector = king - man + woman\n","computed_similarities = []\n","\n","for word in nlp.vocab:\n","    # Ignore words without vectors and mixed-case words:\n","    if word.has_vector:\n","        if word.is_lower:\n","            if word.is_alpha:\n","                similarity = cosine_similarity(new_vector, word.vector)\n","                computed_similarities.append((word, similarity))\n","\n","computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n","\n","print([w[0].text for w in computed_similarities[:10]])"]},{"source":["## Sentiment Analysis"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":153,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":153}],"source":["nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[],"source":["from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","sid = SentimentIntensityAnalyzer()"]},{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}\n{'neg': 0.477, 'neu': 0.523, 'pos': 0.0, 'compound': -0.8074}\n"]}],"source":["a = 'This was the best, most awesome movie EVER MADE!!!'\n","print(sid.polarity_scores(a))\n","b = 'This was the worst film to ever disgrace the screen.'\n","print(sid.polarity_scores(b))"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  label                                             review\n","0   pos  Stuning even for the non-gamer: This sound tra...\n","1   pos  The best soundtrack ever to anything.: I'm rea...\n","2   pos  Amazing!: This soundtrack is my favorite music...\n","3   pos  Excellent Soundtrack: I truly like this soundt...\n","4   pos  Remember, Pull Your Jaw Off The Floor After He..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pos</td>\n      <td>Stuning even for the non-gamer: This sound tra...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pos</td>\n      <td>The best soundtrack ever to anything.: I'm rea...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pos</td>\n      <td>Amazing!: This soundtrack is my favorite music...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pos</td>\n      <td>Excellent Soundtrack: I truly like this soundt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pos</td>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":160}],"source":["df = pd.read_csv('data/amazonreviews.tsv', sep='\\t')\n","df.head()"]},{"cell_type":"code","execution_count":161,"metadata":{},"outputs":[],"source":["# REMOVE NaN VALUES AND EMPTY STRINGS:\n","df.dropna(inplace=True)\n","\n","blanks = []  # start with an empty list\n","\n","for i,lb,rv in df.itertuples():  # iterate over the DataFrame\n","    if type(rv)==str:            # avoid NaN values\n","        if rv.isspace():         # test 'review' for whitespace\n","            blanks.append(i)     # add matching index numbers to the list\n","\n","df.drop(blanks, inplace=True)"]},{"cell_type":"code","execution_count":165,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  label                                             review  \\\n","0   pos  Stuning even for the non-gamer: This sound tra...   \n","1   pos  The best soundtrack ever to anything.: I'm rea...   \n","2   pos  Amazing!: This soundtrack is my favorite music...   \n","3   pos  Excellent Soundtrack: I truly like this soundt...   \n","4   pos  Remember, Pull Your Jaw Off The Floor After He...   \n","\n","                                              scores  compound comp_score  \n","0  {'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...    0.9454        pos  \n","1  {'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...    0.8957        pos  \n","2  {'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...    0.9858        pos  \n","3  {'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...    0.9814        pos  \n","4  {'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...    0.9781        pos  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>review</th>\n      <th>scores</th>\n      <th>compound</th>\n      <th>comp_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pos</td>\n      <td>Stuning even for the non-gamer: This sound tra...</td>\n      <td>{'neg': 0.088, 'neu': 0.669, 'pos': 0.243, 'co...</td>\n      <td>0.9454</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pos</td>\n      <td>The best soundtrack ever to anything.: I'm rea...</td>\n      <td>{'neg': 0.018, 'neu': 0.837, 'pos': 0.145, 'co...</td>\n      <td>0.8957</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pos</td>\n      <td>Amazing!: This soundtrack is my favorite music...</td>\n      <td>{'neg': 0.04, 'neu': 0.692, 'pos': 0.268, 'com...</td>\n      <td>0.9858</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pos</td>\n      <td>Excellent Soundtrack: I truly like this soundt...</td>\n      <td>{'neg': 0.09, 'neu': 0.615, 'pos': 0.295, 'com...</td>\n      <td>0.9814</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pos</td>\n      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n      <td>{'neg': 0.0, 'neu': 0.746, 'pos': 0.254, 'comp...</td>\n      <td>0.9781</td>\n      <td>pos</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":165}],"source":["df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n","df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n","df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n","df.head()"]},{"cell_type":"code","execution_count":167,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0.7091\n","              precision    recall  f1-score   support\n","\n","         neg       0.86      0.51      0.64      5097\n","         pos       0.64      0.91      0.75      4903\n","\n","    accuracy                           0.71     10000\n","   macro avg       0.75      0.71      0.70     10000\n","weighted avg       0.75      0.71      0.70     10000\n","\n"]}],"source":["print(metrics.accuracy_score(df['label'],df['comp_score']))\n","print(metrics.classification_report(df['label'],df['comp_score']))"]},{"source":["## Topic Modeling: LDA"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":171,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Article\n","0  In the Washington of 2016, even when the polic...\n","1    Donald Trump has used Twitter  â€”   his prefe...\n","2    Donald Trump is unabashedly praising Russian...\n","3  Updated at 2:50 p. m. ET, Russian President Vl...\n","4  From photography, illustration and video, to d..."],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the Washington of 2016, even when the polic...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Donald Trump has used Twitter  â€”   his prefe...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Donald Trump is unabashedly praising Russian...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From photography, illustration and video, to d...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":171}],"source":["npr = pd.read_csv('data/npr.csv')\n","npr.head()"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[],"source":["# Preprocessing\n","from sklearn.feature_extraction.text import CountVectorizer\n","# remove frequent words (max_df=0.95) and very rare words (min_df=2)\n","cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","dtm = cv.fit_transform(npr['Article'])"]},{"cell_type":"code","execution_count":174,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LatentDirichletAllocation(n_components=7, random_state=42)"]},"metadata":{},"execution_count":174}],"source":["# LDA\n","from sklearn.decomposition import LatentDirichletAllocation\n","# n_components is the arbitrary number of expected topics\n","LDA = LatentDirichletAllocation(n_components=7,random_state=42)\n","LDA.fit(dtm)"]},{"cell_type":"code","execution_count":175,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["THE TOP 15 WORDS FOR TOPIC #0\n","['companies', 'money', 'year', 'federal', '000', 'new', 'percent', 'government', 'company', 'million', 'care', 'people', 'health', 'said', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #1\n","['military', 'house', 'security', 'russia', 'government', 'npr', 'reports', 'says', 'news', 'people', 'told', 'police', 'president', 'trump', 'said']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #2\n","['way', 'world', 'family', 'home', 'day', 'time', 'water', 'city', 'new', 'years', 'food', 'just', 'people', 'like', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #3\n","['time', 'new', 'don', 'years', 'medical', 'disease', 'patients', 'just', 'children', 'study', 'like', 'women', 'health', 'people', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #4\n","['voters', 'vote', 'election', 'party', 'new', 'obama', 'court', 'republican', 'campaign', 'people', 'state', 'president', 'clinton', 'said', 'trump']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #5\n","['years', 'going', 've', 'life', 'don', 'new', 'way', 'music', 'really', 'time', 'know', 'think', 'people', 'just', 'like']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #6\n","['student', 'years', 'data', 'science', 'university', 'people', 'time', 'schools', 'just', 'education', 'new', 'like', 'students', 'school', 'says']\n","\n","\n"]}],"source":["# Get top words by components (topics)\n","for index,topic in enumerate(LDA.components_):\n","    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n","    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])\n","    print('\\n')"]},{"cell_type":"code","execution_count":176,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Article  Topic\n","0  In the Washington of 2016, even when the polic...      1\n","1    Donald Trump has used Twitter  â€”   his prefe...      1\n","2    Donald Trump is unabashedly praising Russian...      1\n","3  Updated at 2:50 p. m. ET, Russian President Vl...      1\n","4  From photography, illustration and video, to d...      2\n","5  I did not want to join yoga class. I hated tho...      3\n","6  With a   who has publicly supported the debunk...      3\n","7  I was standing by the airport exit, debating w...      2\n","8  If movies were trying to be more realistic, pe...      3\n","9  Eighteen years ago, on New Yearâ€™s Eve, David F...      2"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n      <th>Topic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the Washington of 2016, even when the polic...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Donald Trump has used Twitter  â€”   his prefe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Donald Trump is unabashedly praising Russian...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From photography, illustration and video, to d...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I did not want to join yoga class. I hated tho...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>With a   who has publicly supported the debunk...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I was standing by the airport exit, debating w...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>If movies were trying to be more realistic, pe...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Eighteen years ago, on New Yearâ€™s Eve, David F...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":176}],"source":["# Reasign topics to the dataset\n","topic_results = LDA.transform(dtm)\n","npr['Topic'] = topic_results.argmax(axis=1)\n","npr.head(10)"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[],"source":["mytopic_dict = {0:'economy',1:'politics',2:'local',3:'health',4:'election',5:'music',6:'education'}"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[],"source":["npr['Topic'] = npr['Topic'].map(mytopic_dict)"]},{"source":["## Topic Modeling: Non-negative Matrix Factorization"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#npr = pd.read_csv('data/npr.csv')\n","#npr.head()"]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[],"source":["# Preprocessing\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# remove frequent words (max_df=0.95) and very rare words (min_df=2)\n","tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n","dtm = tfidf.fit_transform(npr['Article'])"]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["NMF(n_components=7, random_state=42)"]},"metadata":{},"execution_count":178}],"source":["# NMF\n","from sklearn.decomposition import NMF\n","# n_components is the arbitrary number of expected topics\n","nmf_model = NMF(n_components=7,random_state=42)\n","nmf_model.fit(dtm)"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["THE TOP 15 WORDS FOR TOPIC #0\n","['new', 'research', 'like', 'patients', 'health', 'disease', 'percent', 'women', 'virus', 'study', 'water', 'food', 'people', 'zika', 'says']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #1\n","['gop', 'pence', 'presidential', 'russia', 'administration', 'election', 'republican', 'obama', 'white', 'house', 'donald', 'campaign', 'said', 'president', 'trump']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #2\n","['senate', 'house', 'people', 'act', 'law', 'tax', 'plan', 'republicans', 'affordable', 'obamacare', 'coverage', 'medicaid', 'insurance', 'care', 'health']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #3\n","['officers', 'syria', 'security', 'department', 'law', 'isis', 'russia', 'government', 'state', 'attack', 'president', 'reports', 'court', 'said', 'police']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #4\n","['primary', 'cruz', 'election', 'democrats', 'percent', 'party', 'delegates', 'vote', 'state', 'democratic', 'hillary', 'campaign', 'voters', 'sanders', 'clinton']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #5\n","['love', 've', 'don', 'album', 'way', 'time', 'song', 'life', 'really', 'know', 'people', 'think', 'just', 'music', 'like']\n","\n","\n","THE TOP 15 WORDS FOR TOPIC #6\n","['teacher', 'state', 'high', 'says', 'parents', 'devos', 'children', 'college', 'kids', 'teachers', 'student', 'education', 'schools', 'school', 'students']\n","\n","\n"]}],"source":["# Get top words by components (topics)\n","for index,topic in enumerate(nmf_model.components_):\n","    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n","    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n","    print('\\n')"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["# Reasign topics to the dataset\n","topic_results = nmf_model.transform(dtm)\n","npr['Topic2'] = topic_results.argmax(axis=1)"]},{"cell_type":"code","execution_count":188,"metadata":{},"outputs":[],"source":["mytopic_dict2 = {0:'health',1:'election',2:'legislation',3:'politics',4:'election',5:'music',6:'education'}"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                             Article     Topic     Topic2\n","0  In the Washington of 2016, even when the polic...  politics   election\n","1    Donald Trump has used Twitter  â€”   his prefe...  politics   election\n","2    Donald Trump is unabashedly praising Russian...  politics   election\n","3  Updated at 2:50 p. m. ET, Russian President Vl...  politics   politics\n","4  From photography, illustration and video, to d...     local  education\n","5  I did not want to join yoga class. I hated tho...    health      music\n","6  With a   who has publicly supported the debunk...    health     health\n","7  I was standing by the airport exit, debating w...     local     health\n","8  If movies were trying to be more realistic, pe...    health     health\n","9  Eighteen years ago, on New Yearâ€™s Eve, David F...     local      music"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Article</th>\n      <th>Topic</th>\n      <th>Topic2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the Washington of 2016, even when the polic...</td>\n      <td>politics</td>\n      <td>election</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Donald Trump has used Twitter  â€”   his prefe...</td>\n      <td>politics</td>\n      <td>election</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Donald Trump is unabashedly praising Russian...</td>\n      <td>politics</td>\n      <td>election</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n      <td>politics</td>\n      <td>politics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From photography, illustration and video, to d...</td>\n      <td>local</td>\n      <td>education</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I did not want to join yoga class. I hated tho...</td>\n      <td>health</td>\n      <td>music</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>With a   who has publicly supported the debunk...</td>\n      <td>health</td>\n      <td>health</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I was standing by the airport exit, debating w...</td>\n      <td>local</td>\n      <td>health</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>If movies were trying to be more realistic, pe...</td>\n      <td>health</td>\n      <td>health</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Eighteen years ago, on New Yearâ€™s Eve, David F...</td>\n      <td>local</td>\n      <td>music</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":189}],"source":["npr['Topic2'] = npr['Topic2'].map(mytopic_dict2)\n","npr.head(10)"]},{"source":["## Summarization"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from string import punctuation\n","nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["doc =\"\"\"Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["docx = nlp(doc)\n","extra_words = list(STOP_WORDS) + list(punctuation) + ['\\n']"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Word frequency\n","all_words = [word.text for word in docx]\n","\n","Freq_word = {}\n","for w in all_words:\n","    w1 = w.lower()\n","    if w1 not in extra_words and w1.isalpha():\n","        if w1 in Freq_word.keys():\n","            Freq_word[w1] += 1\n","        else:\n","            Freq_word[w1] = 1\n","#Freq_word"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Topic of document given:\nmachine learning data "]}],"source":["# Main topics\n","val = sorted(Freq_word.values())\n","max_freq = val[-3:]\n","print(\"Topic of document given:\")\n","for word,freq in Freq_word.items():  \n","    \n","    if freq in max_freq:\n","        print(word, end=\" \")\n","        \n","    else:\n","        continue"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# TF-IDF\n","for word in Freq_word.keys():  \n","        Freq_word[word] = (Freq_word[word] / max_freq[-1])\n","#Freq_word"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Sentence Strength (score)\n","sent_strength = {}\n","for sent in docx.sents:\n","    for word in sent :\n","       \n","        if word.text.lower() in Freq_word.keys():\n","            \n","            if sent in sent_strength.keys():\n","                sent_strength[sent]+=Freq_word[word.text.lower()]\n","            else:\n","                sent_strength[sent]=Freq_word[word.text.lower()]\n","        else:\n","            continue\n","#sent_strength"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Sort Sentences\n","top_sentences = (sorted(sent_strength.values())[::-1])\n","top20percent_sentence=int(0.2 * len(top_sentences))\n","top_sent=top_sentences[:top20percent_sentence]"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task."]}],"source":["# Summary\n","summary=[]\n","for sent,strength in sent_strength.items():  \n","    if strength in top_sent:\n","        summary.append(sent)\n","        \n","    else:\n","        continue\n","\n","for i in summary:\n","    print(i,end=\"\")"]},{"source":["### Compare result with gensim summarizer"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.'"]},"metadata":{},"execution_count":26}],"source":["from gensim.summarization import summarize\n","summarize(doc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#https://towardsdatascience.com/7-models-on-huggingface-you-probably-didnt-knew-existed-f3d079a4fd7c"]}]}